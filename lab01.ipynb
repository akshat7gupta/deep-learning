{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOadff2niueXc3BYCOF2UVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshat7gupta/deep-learning/blob/main/lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4UKZw50YKIW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as dt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make threshold a -ve value if you want to run exactly\n",
        "# max_iterations.\n",
        "def gradient_descent(max_iterations,threshold,w_init,\n",
        "                     obj_func,grad_func,extra_param = [],\n",
        "                     learning_rate=0.05,momentum=0.8):\n",
        "\n",
        "    w = w_init\n",
        "    w_history = w\n",
        "    f_history = obj_func(w,extra_param)\n",
        "    delta_w = np.zeros(w.shape)\n",
        "    i = 0\n",
        "    diff = 1.0e10\n",
        "\n",
        "    while  i<max_iterations and diff>threshold:\n",
        "        delta_w = -learning_rate*grad_func(w,extra_param) + momentum*delta_w\n",
        "        w = w+delta_w\n",
        "\n",
        "        # store the history of w and f\n",
        "        w_history = np.vstack((w_history,w))\n",
        "        f_history = np.vstack((f_history,obj_func(w,extra_param)))\n",
        "\n",
        "        # update iteration number and diff between successive values\n",
        "        # of objective function\n",
        "        i+=1\n",
        "        diff = np.absolute(f_history[-1]-f_history[-2])\n",
        "\n",
        "    return w_history,f_history"
      ],
      "metadata": {
        "id": "WDC7f1IEYWas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_fw():\n",
        "    xcoord = np.linspace(-10.0,10.0,50)\n",
        "    ycoord = np.linspace(-10.0,10.0,50)\n",
        "    w1,w2 = np.meshgrid(xcoord,ycoord)\n",
        "    pts = np.vstack((w1.flatten(),w2.flatten()))\n",
        "\n",
        "    # All 2D points on the grid\n",
        "    pts = pts.transpose()\n",
        "\n",
        "    # Function value at each point\n",
        "    f_vals = np.sum(pts*pts,axis=1)\n",
        "    function_plot(pts,f_vals)\n",
        "    plt.title('Objective Function Shown in Color')\n",
        "    plt.show()\n",
        "    return pts,f_vals\n",
        "\n",
        "# Helper function to annotate a single point\n",
        "def annotate_pt(text,xy,xytext,color):\n",
        "    plt.plot(xy[0],xy[1],marker='P',markersize=10,c=color)\n",
        "    plt.annotate(text,xy=xy,xytext=xytext,\n",
        "                 # color=color,\n",
        "                 arrowprops=dict(arrowstyle=\"->\",\n",
        "                 color = color,\n",
        "                 connectionstyle='arc3'))\n",
        "\n",
        "# Plot the function\n",
        "# Pts are 2D points and f_val is the corresponding function value\n",
        "def function_plot(pts,f_val):\n",
        "    f_plot = plt.scatter(pts[:,0],pts[:,1],\n",
        "                         c=f_val,vmin=min(f_val),vmax=max(f_val),\n",
        "                         cmap='RdBu_r')\n",
        "    plt.colorbar(f_plot)\n",
        "    # Show the optimal point\n",
        "    annotate_pt('global minimum',(0,0),(-5,-7),'yellow')\n",
        "\n",
        "pts,f_vals = visualize_fw()"
      ],
      "metadata": {
        "id": "R1D3mLi-YdIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective function\n",
        "def f(w,extra=[]):\n",
        "    return np.sum(w*w)\n",
        "\n",
        "# Function to compute the gradient\n",
        "def grad(w,extra=[]):\n",
        "    return 2*w\n",
        "\n",
        "# Function to plot the objective function\n",
        "# and learning history annotated by arrows\n",
        "# to show how learning proceeded\n",
        "def visualize_learning(w_history):\n",
        "\n",
        "    # Make the function plot\n",
        "    function_plot(pts,f_vals)\n",
        "\n",
        "    # Plot the history\n",
        "    plt.plot(w_history[:,0],w_history[:,1],marker='o',c='magenta')\n",
        "\n",
        "    # Annotate the point found at last iteration\n",
        "    annotate_pt('minimum found',\n",
        "                (w_history[-1,0],w_history[-1,1]),\n",
        "                (-1,7),'green')\n",
        "    iter = w_history.shape[0]\n",
        "    for w,i in zip(w_history,range(iter-1)):\n",
        "        # Annotate with arrows to show history\n",
        "        plt.annotate(\"\",\n",
        "                    xy=w, xycoords='data',\n",
        "                    xytext=w_history[i+1,:], textcoords='data',\n",
        "                    arrowprops=dict(arrowstyle='<-',\n",
        "                            connectionstyle='angle3'))\n",
        "\n",
        "def solve_fw():\n",
        "    # Setting up\n",
        "    rand = np.random.RandomState(19)\n",
        "    w_init = rand.uniform(-10,10,2)\n",
        "    fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(18, 12))\n",
        "    learning_rates = [0.05,0.2,0.5,0.8]\n",
        "    momentum = [0,0.5,0.9]\n",
        "    ind = 1\n",
        "\n",
        "    # Iteration through all possible parameter combinations\n",
        "    for alpha in momentum:\n",
        "        for eta,col in zip(learning_rates,[0,1,2,3]):\n",
        "            plt.subplot(3,4,ind)\n",
        "            w_history,f_history = gradient_descent(5,-1,w_init, f,grad,[],eta,alpha)\n",
        "\n",
        "            visualize_learning(w_history)\n",
        "            ind = ind+1\n",
        "            plt.text(-9, 12,'Learning Rate = '+str(eta),fontsize=13)\n",
        "            if col==1:\n",
        "                plt.text(10,15,'momentum = ' + str(alpha),fontsize=20)\n",
        "\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KBYKNaz2YiU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}